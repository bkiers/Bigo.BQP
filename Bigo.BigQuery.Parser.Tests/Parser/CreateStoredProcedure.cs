using Xunit;

namespace Bigo.BigQuery.Parser.Tests.Parser;

public class CreateStoredProcedure : BaseParserTest
{
    [Theory]
    [InlineData("CREATE PROCEDURE my_bq_project.my_dataset.spark_proc()\nWITH CONNECTION `my-project-id.us.my-connection`\nOPTIONS(engine=\"SPARK\", runtime_version=\"2.2\", main_file_uri=\"gs://my-bucket/my-pyspark-main.py\")\nLANGUAGE PYTHON")]
    [InlineData("CREATE OR REPLACE PROCEDURE my_bq_project.my_dataset.spark_proc()\nWITH CONNECTION `my-project-id.us.my-connection`\nOPTIONS(engine=\"SPARK\", runtime_version=\"2.2\")\nLANGUAGE PYTHON AS R\"\"\"\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"spark-bigquery-demo\").getOrCreate()\n\n# Load data from BigQuery.\nwords = spark.read.format(\"bigquery\") \\\n  .option(\"table\", \"bigquery-public-data:samples.shakespeare\") \\\n  .load()\nwords.createOrReplaceTempView(\"words\")\n\n# Perform word count.\nword_count = words.select('word', 'word_count').groupBy('word').sum('word_count').withColumnRenamed(\"sum(word_count)\", \"sum_word_count\")\nword_count.show()\nword_count.printSchema()\n\n# Saving the data to BigQuery\nword_count.write.format(\"bigquery\") \\\n  .option(\"writeMethod\", \"direct\") \\\n  .save(\"wordcount_dataset.wordcount_output\")\n\"\"\"\n")]
    [InlineData("CREATE OR REPLACE PROCEDURE my_bq_project.my_dataset.spark_proc(num INT64)\nWITH CONNECTION `my-project-id.us.my-connection`\nOPTIONS(engine=\"SPARK\", runtime_version=\"2.2\")\nLANGUAGE PYTHON AS R\"\"\"\nfrom pyspark.sql import SparkSession\nimport os\nimport json\n\nspark = SparkSession.builder.appName(\"spark-bigquery-demo\").getOrCreate()\nsc = spark.sparkContext\n\n# Get the input parameter num in JSON string and convert to a Python variable\nnum = int(json.loads(os.environ[\"BIGQUERY_PROC_PARAM.num\"]))\n\n\"\"\"")]
    [InlineData("CREATE OR REPLACE PROCEDURE my_bq_project.my_dataset.spark_proc(num INT64, info ARRAY<STRUCT<a INT64, b STRING>>)\nWITH CONNECTION `my-project-id.us.my-connection`\nOPTIONS(engine=\"SPARK\", runtime_version=\"2.2\")\nLANGUAGE PYTHON AS R\"\"\"\nfrom pyspark.sql import SparkSession\nfrom bigquery.spark.procedure import SparkProcParamContext\n\ndef check_in_param(x, num):\n  return x['a'] + num\n\ndef main():\n  spark = SparkSession.builder.appName(\"spark-bigquery-demo\").getOrCreate()\n  sc=spark.sparkContext\n  spark_proc_param_context = SparkProcParamContext.getOrCreate(spark)\n\n  # Get the input parameter num of type INT64\n  num = spark_proc_param_context.num\n\n  # Get the input parameter info of type ARRAY<STRUCT<a INT64, b STRING>>\n  info = spark_proc_param_context.info\n\n  # Pass the parameter to executors\n  df = sc.parallelize(info)\n  value = df.map(lambda x : check_in_param(x, num)).sum()\n\nmain()\n\"\"\"")]
    [InlineData("CREATE OR REPLACE PROCEDURE my_bq_project.my_dataset.pyspark_proc(IN int INT64, INOUT datetime DATETIME,OUT b BOOL, OUT info ARRAY<STRUCT<a INT64, b STRING>>, OUT time TIME, OUT f FLOAT64, OUT bs BYTES, OUT date DATE, OUT ts TIMESTAMP, OUT js JSON)\nWITH CONNECTION `my_bq_project.my_dataset.my_connection`\nOPTIONS(engine=\"SPARK\", runtime_version=\"2.2\") LANGUAGE PYTHON AS\nR\"\"\"\nfrom pyspark.sql.session import SparkSession\nimport datetime\nfrom bigquery.spark.procedure import SparkProcParamContext\n\nspark = SparkSession.builder.appName(\"bigquery-pyspark-demo\").getOrCreate()\nspark_proc_param_context = SparkProcParamContext.getOrCreate(spark)\n\n# Reading the IN and INOUT parameter values.\nint = spark_proc_param_context.int\ndt = spark_proc_param_context.datetime\nprint(\"IN parameter value: \", int, \", INOUT parameter value: \", dt)\n\n# Returning the value of the OUT and INOUT parameters.\nspark_proc_param_context.datetime = datetime.datetime(1970, 1, 1, 0, 20, 0, 2, tzinfo=datetime.timezone.utc)\nspark_proc_param_context.b = True\nspark_proc_param_context.info = [{\"a\":2, \"b\":\"dd\"}, {\"a\":2, \"b\":\"dd\"}]\nspark_proc_param_context.time = datetime.time(23, 20, 50, 520000)\nspark_proc_param_context.f = 20.23\nspark_proc_param_context.bs = b\"hello\"\nspark_proc_param_context.date = datetime.date(1985, 4, 12)\nspark_proc_param_context.ts = datetime.datetime(1970, 1, 1, 0, 20, 0, 2, tzinfo=datetime.timezone.utc)\nspark_proc_param_context.js = {\"name\": \"Alice\", \"age\": 30}\n\"\"\"")]
    [InlineData("CREATE OR REPLACE PROCEDURE my_bq_project.my_dataset.spark_proc()\nWITH CONNECTION `my-project-id.us.my-connection`\nOPTIONS(engine=\"SPARK\", runtime_version=\"2.2\")\nLANGUAGE PYTHON AS R\"\"\"\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n   .builder \\\n   .appName(\"Python Spark SQL Dataproc Hive Metastore integration test example\") \\\n   .enableHiveSupport() \\\n   .getOrCreate()\n\nspark.sql(\"CREATE DATABASE IF NOT EXISTS records\")\n\nspark.sql(\"CREATE TABLE IF NOT EXISTS records.student (eid int, name String, score int)\")\n\nspark.sql(\"INSERT INTO records.student VALUES (1000000, 'AlicesChen', 10000)\")\n\ndf = spark.sql(\"SELECT * FROM records.student\")\n\ndf.write.format(\"bigquery\") \\\n  .option(\"writeMethod\", \"direct\") \\\n  .save(\"records_dataset.student\")\n\"\"\"\n")]
    public void Test(string input)
    {
        ParseAllTokens(input, parser => parser.create_stored_procedure());
    }
}